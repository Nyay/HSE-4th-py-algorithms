{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.externals import joblib\nimport nltk\nimport spacy\nfrom tqdm import tqdm_notebook\n\nfrom sklearn import metrics\n\nimport torch as tt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torchtext.data import Field, LabelField, BucketIterator, ReversibleField, TabularDataset, Iterator\n\nSEED = 42\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c231380859fe81461206d89eb2fcbfca6884aeb1"
      },
      "cell_type": "code",
      "source": "spacy_en = spacy.load('en')\nspacy_en.remove_pipe('tagger')\nspacy_en.remove_pipe('ner')\n\ndef tokenizer(text):\n    return [tok.lemma_ for tok in spacy_en.tokenizer(text)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "782f7ef0ca14084202fa686b09f9220d32961195"
      },
      "cell_type": "code",
      "source": "dataframe = pd.read_csv('../input/train-balanced-sarcasm.csv')\ndataframe = dataframe.dropna()\ndataframe = dataframe[['label', 'comment']]\ndataframe.to_csv('data.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "148edae56692fcdf90e7e16345de6cab70d07458"
      },
      "cell_type": "code",
      "source": "classes={'0':0,\n         '1':1}\n\nTEXT = Field(include_lengths=True,\n             batch_first=True, \n             tokenize=tokenizer,\n             eos_token='<eos>',\n             lower=True,\n             stop_words=nltk.corpus.stopwords.words('english'))\n\nLABEL = LabelField(dtype=tt.int64,\n                   use_vocab=True,\n                   preprocessing=lambda x: classes[x])\n\ndataset = TabularDataset('data.csv',\n                         format='csv', \n                         fields=[('label', LABEL),('comment', TEXT)], \n                         skip_header=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a43b2c4e6265a2861497f94c48df326a8010f359"
      },
      "cell_type": "code",
      "source": "TEXT.build_vocab(dataset, min_freq=5, vectors=\"glove.6B.300d\")\nLABEL.build_vocab(dataset)\n\nembed_matrix = TEXT.vocab.vectors",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b685747087fe84b0863613b141a2a4f2c7f7c2d4"
      },
      "cell_type": "code",
      "source": "train, test = dataset.split(0.8, stratified=True)\ntrain, valid = train.split(0.9, stratified=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "1a5960d0f59b893cc58fb15a62149fa192edbed7"
      },
      "cell_type": "code",
      "source": "class RNNModel(nn.Module):\n    \n    def __init__(self, vocab_size, embed_size, hidden_size, embed_matrix):\n        \n        super(RNNModel, self).__init__()\n\n        self.embedding = nn.Embedding.from_pretrained(embed_matrix, freeze=False)\n        \n        self.rnn = nn.LSTM(input_size=embed_size,\n                           hidden_size=hidden_size,\n                           bidirectional=True,\n                           batch_first=True\n                          )\n        \n        self.fc = nn.Linear(hidden_size * 2 * 2, 2)\n        \n        \n    def forward(self, batch):\n        \n        x, x_len = batch.comment\n        \n        x = x.cuda()\n        batch.label = batch.label.cuda()\n        \n        x = self.embedding(x)\n\n        if x_len is not None:\n            x_len = x_len.view(-1).tolist()\n            x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True)\n            \n        _, (hidden, cell) = self.rnn(x)\n        \n        hidden, cell = hidden.transpose(0,1), cell.transpose(0,1)\n\n        hidden = hidden.contiguous().view(hidden.size(0),-1)\n        cell = cell.contiguous().view(cell.size(0),-1)\n        \n        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n        x = self.fc(x)\n\n        return x",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01eb185bf3ff84ce92ef58cf9d10adbcfbf2b1b2"
      },
      "cell_type": "code",
      "source": "tt.cuda.empty_cache()\n\nbatch_size = 600\n\nmodel = RNNModel(len(TEXT.vocab.itos),\n                embed_size=300,\n                embed_matrix=embed_matrix,\n                hidden_size=600)\n\nmodel = model.cuda()\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train, valid, test),\n    batch_sizes=(batch_size, batch_size, batch_size),\n    shuffle=True,\n    sort_key=lambda x: len(x.comment),\n    sort_within_batch=True\n)\n\noptimizer = optim.Adam(model.parameters())\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\ncriterion = nn.CrossEntropyLoss()",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82426693a6162b83afc2342a07a66f39bbabd6c4"
      },
      "cell_type": "code",
      "source": "def calculate_accuracy(predictions, Y):\n    \n    values, indices = tt.max(predictions.data, 1)\n    acc = (indices == Y).sum().data.cpu().numpy()\n    \n    return acc / indices.size()[0]\n\n\ndef evaluate_test_accuracy(model, test_iterator, criterion):\n    \n    model.eval()\n    \n    epoch_accuracy, n_batches = 0, len(test_iterator)\n    \n    with tt.no_grad():\n        for batch in test_iterator:\n            \n            pred = model(batch)\n            \n            accuracy = calculate_accuracy(pred, batch.label)\n            epoch_accuracy += accuracy.item()\n        \n    return epoch_accuracy / n_batches",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc60d0bb1f54ae4ced9a3673daa3200c4f4678c6"
      },
      "cell_type": "code",
      "source": "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n    model.train()\n\n    running_loss, epoch_acc = 0, list()\n\n    n_batches = len(iterator)\n    iterator = tqdm_notebook(iterator,\n                             total=n_batches,\n                             desc='Current epoch – %d' % (curr_epoch),\n                             leave=True)\n\n    for i, batch in enumerate(iterator):\n        \n        optimizer.zero_grad()\n\n        pred = model(batch)\n        loss = criterion(pred, batch.label)\n        loss.backward()\n        optimizer.step()\n\n        curr_loss = loss.data.cpu().detach().item()\n        \n        loss_smoothing = i / (i+1)\n        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n\n        iterator.set_postfix(loss='%.5f' % running_loss)\n        \n        accuracy = calculate_accuracy(pred, batch.label)\n        epoch_acc.append(accuracy.item())\n\n    return running_loss, np.mean(epoch_acc)\n\ndef _test_epoch(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss, epoch_acc = 0, list()\n    \n    n_batches = len(iterator)\n    \n    with tt.no_grad():\n        for batch in iterator:\n            pred = model(batch)\n            loss = criterion(pred, batch.label)\n            epoch_loss += loss.data.item()\n            \n            accuracy = calculate_accuracy(pred, batch.label)\n            epoch_acc.append(accuracy.item())\n\n    return epoch_loss / n_batches, np.mean(epoch_acc)\n\n\ndef nn_train(model, train_iterator, valid_iterator, criterion, optimizer, test_iterator, n_epochs=100,\n             scheduler=None, early_stopping=0):\n\n    prev_loss = 100500\n    es_epochs = 0\n    best_epoch = None\n    history = pd.DataFrame()\n\n    for epoch in range(n_epochs):\n        train_loss, train_acc = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n        valid_loss, valid_acc = _test_epoch(model, valid_iterator, criterion)\n\n        valid_loss = valid_loss\n        print('Validation loss – %.5f | Accuracy score – %.5f' % (valid_loss, valid_acc))\n\n        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n        history = history.append(record, ignore_index=True)\n\n        if early_stopping > 0:\n            if valid_loss > prev_loss:\n                es_epochs += 1\n            else:\n                es_epochs = 0\n\n            if es_epochs >= early_stopping:\n                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n                line = 'Training early stopping!\\nBest epoch – %d\\nVal – %.5f'\n                print(line % (best_epoch['epoch'],\n                              best_epoch['valid_loss']))\n                break\n\n            prev_loss = min(prev_loss, valid_loss)",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1880ac2a3c7544df04ff15f05687ddf96e78c797",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "nn_train(model, train_iterator, valid_iterator, criterion, optimizer, test_iterator, scheduler=scheduler, \n         n_epochs=10, early_stopping=2)",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, description='Current epoch – 0', max=1213, style=ProgressStyle(description…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52e8e2c5d99647cc80191fa1530ba8a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "Validation loss – 0.54862 | Accuracy score – 0.71653\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, description='Current epoch – 1', max=1213, style=ProgressStyle(description…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de304e28c13a4b069a9a2fce9040b7d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "Validation loss – 0.54533 | Accuracy score – 0.72184\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, description='Current epoch – 2', max=1213, style=ProgressStyle(description…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1a91da7323942c3a218cf68cbd5eac6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "Validation loss – 0.55840 | Accuracy score – 0.71908\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, description='Current epoch – 3', max=1213, style=ProgressStyle(description…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8de9febb9a54f559f52681e3d457c9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "Validation loss – 0.60437 | Accuracy score – 0.71022\nTraining early stopping!\nBest epoch – 1\nVal – 0.54533\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22a3fa3be69d3fddd379384aa4a539927ece3853"
      },
      "cell_type": "code",
      "source": "print('Final Test Accuracy – %.5f' % evaluate_test_accuracy(model,\n                                                     test_iterator,\n                                                     criterion))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}